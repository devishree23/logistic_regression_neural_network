{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class**: CT5133 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student id**: 20230852"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Implement Logistic Regression\n",
    "Logistic Regression is a classification algorithm used to classify categorical data. The Sigmoid function or the Hypothesis function to calculate the probabilities of the input data passed to the function. In this implementation of the algorithm, Stochastic Gradient Descent is used to determine model's parameter values that minimises the cost occurred as much as possible to reach the local minima [1]. In Stochastic Gradient Descent, we select one sample from the training cases at random and update weight and bias values as per that case. There are two hyper-parameters learning rate (alpha) and the number of epochs which are passed to the fit function. The size of the steps we take to reach the local minima is decided by the learning rate alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X,w,b):\n",
    "    # Weighted Sum function takes in the feature list (X) along with weights (w) &  \n",
    "    # bias (b) and returns the value from below calculation\n",
    "    \n",
    "    y = np.dot(w.T,X)+b\n",
    "    return float(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    # The Sigmoid function calculates the probabilities of the input data passed\n",
    "    # to the function. The output of this function will be a value between 0\n",
    "    # and 1.\n",
    "    \n",
    "    z = 1 / (1 + np.exp(-y))\n",
    "    return z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,Y,a,epochs):\n",
    "    # The fit function takes in the features list and label from the train dataset \n",
    "    # along with the learning rate (alpha) and epochs. The features and labels of \n",
    "    # the validation set is also taken as input. Stochastic Gradient descent is \n",
    "    # implemented to update model parameters - weights and bias. \n",
    "    \n",
    "    #initialization\n",
    "    np.random.seed(50) #for ensuring data can be reproduced\n",
    "    \n",
    "    alpha = a\n",
    "    max_iterations = epochs\n",
    "\n",
    "    w = np.random.rand(len(X.columns),1)\n",
    "    b = np.random.rand() \n",
    "    del_w = np.empty(w.shape)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        #selecting 1 random training example for Stochastic Gradient Descent\n",
    "        random_index = np.random.randint(0, len(X), 1) \n",
    "        \n",
    "        #extracting 1 row from training example based on index found above \n",
    "        x = X.iloc[random_index,:]\n",
    "        y = Y.iloc[random_index,:]\n",
    "        \n",
    "        x=x.squeeze()\n",
    "        y=np.array(y)\n",
    "\n",
    "        #Forward propogation calculation\n",
    "        z = sigmoid(weighted_sum(x,w,b))\n",
    "        \n",
    "        if (z>=0.5):\n",
    "            y_hat = 1\n",
    "        else: y_hat = 0\n",
    "                \n",
    "        #Gradient Descent calculation\n",
    "        for j in range(len(w)):\n",
    "            del_w[j] = (y_hat - y) * x[j]\n",
    "        del_b = (y_hat - y)\n",
    "        for j in range(len(w)):\n",
    "            w[j] = w[j] - alpha * del_w[j]\n",
    "        b = b - alpha * del_b\n",
    "                \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    # Predict function takes in the feature list passed to the function. The theta \n",
    "    # value in the argument is a list which contains the model parameters - weights \n",
    "    # and bias. This function returns the predicted label value based on the \n",
    "    # probability returned by the sigmoid function.\n",
    "    \n",
    "    #extracting weights and bias from theta\n",
    "    w = theta[0]\n",
    "    b = theta[1]\n",
    "    y_hat = np.empty((len(X),1))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        z = sigmoid(weighted_sum(X.iloc[i,:],w,b))\n",
    "        if (z>=0.5):\n",
    "            y_hat[i] = 1\n",
    "        else: y_hat[i] = 0\n",
    "            \n",
    "    return y_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Checking implementation on 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardScaler(X):\n",
    "    # StandardScaler function performs normalization on values in X and return \n",
    "    # new X dataframe \n",
    "    \n",
    "    X_std = pd.DataFrame(X)\n",
    "    n = X.shape[0]\n",
    "    for col in range(X.shape[1]):\n",
    "        col_sum = X.iloc[:,col].sum()\n",
    "        col_mean = X.iloc[:,col].mean()\n",
    "        sd = X.iloc[:,col].std()\n",
    "        for row in range(X.shape[0]):\n",
    "            x =  X.iloc[row,col]\n",
    "            y = (x - col_mean)/sd\n",
    "            X_std.iloc[row,col] = y\n",
    "    \n",
    "    return X_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Moons CSV file as a dataframe\n",
    "moons_data = pd.read_csv(\"moons400.csv\")\n",
    "\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y_moons = moons_data['Class'].values\n",
    "y_moons=pd.DataFrame(y_moons)\n",
    "\n",
    "# The x values are all other columns\n",
    "del moons_data['Class']   # drop the 'Class' column from the dataframe\n",
    "X_moons = moons_data.values # convert the remaining columns to a numpy array\n",
    "\n",
    "#normalizing feature list using defined Standard Scalar function\n",
    "scaled_X_moons = StandardScaler(pd.DataFrame(X_moons)) \n",
    "\n",
    "X_train_val_moons,X_test_moons,y_train_val_moons,y_test_moons=train_test_split(\n",
    "                                                                scaled_X_moons,\n",
    "                                                                y_moons,\n",
    "                                                                test_size=0.15,\n",
    "                                                                random_state=50)\n",
    "X_train_moons,X_val_moons,y_train_moons,y_val_moons=train_test_split(X_train_val_moons,\n",
    "                                                        y_train_val_moons,\n",
    "                                                        test_size=0.15,\n",
    "                                                        random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "\n",
      "Validation Accuracy =  92.16 %\n",
      "Test Accuracy =  83.33 %\n"
     ]
    }
   ],
   "source": [
    "## For Moons dataset\n",
    "alpha=0.001\n",
    "epochs = 10000\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_moons = fit(X_train_moons,y_train_moons,alpha,epochs)\n",
    "print()\n",
    "\n",
    "#VALIDATION\n",
    "y_pred_val_moons = predict(X_val_moons,theta_moons)\n",
    "\n",
    "#TESTING\n",
    "y_pred_test_moons = predict(X_test_moons,theta_moons)\n",
    "\n",
    "simple_val_moons_accr = metrics.accuracy_score(y_val_moons, y_pred_val_moons)*100\n",
    "print(\"Validation Accuracy = \",round(simple_val_moons_accr,2),\"%\")\n",
    "\n",
    "simple_test_moons_accr = metrics.accuracy_score(y_test_moons, y_pred_test_moons)*100\n",
    "print(\"Test Accuracy = \",round(simple_test_moons_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For Moons dataset, the data is not linearly separable. The model gives 92% accuracy on validation data and only 83% accuracy on test data. The model is overfitting the data a little bit as the validation accuracy is a bit higher than the test accuracy.\n",
    "\n",
    "The model gave best accuracy when following hyperparameters were used:<br>\n",
    "Learning rate (alpha) = 0.001<br>\n",
    "Epochs = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Blobs CSV file as a dataframe\n",
    "blobs_data = pd.read_csv(\"blobs250.csv\")\n",
    "\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y_blobs = blobs_data['Class'].values\n",
    "y_blobs=pd.DataFrame(y_blobs)\n",
    "\n",
    "# The x values are all other columns\n",
    "del blobs_data['Class']   # drop the 'Class' column from the dataframe\n",
    "X_blobs = blobs_data.values     # convert the remaining columns to a numpy array\n",
    "\n",
    "#normalizing feature list using defined Standard Scalar function\n",
    "scaled_X_blobs = StandardScaler(pd.DataFrame(X_blobs))  \n",
    "\n",
    "X_train_val_blobs, X_test_blobs, y_train_val_blobs, y_test_blobs = train_test_split(\n",
    "                                                                    scaled_X_blobs,\n",
    "                                                                    y_blobs,\n",
    "                                                                    test_size=0.15,\n",
    "                                                                    random_state=50)\n",
    "X_train_blobs, X_val_blobs, y_train_blobs, y_val_blobs=train_test_split(\n",
    "                                                            X_train_val_blobs,\n",
    "                                                            y_train_val_blobs,\n",
    "                                                            test_size=0.15,\n",
    "                                                            random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "\n",
      "Validation Accuracy =  100.0 %\n",
      "Test Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "## For Blobs dataset\n",
    "alpha = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_blobs = fit(X_train_blobs,y_train_blobs,alpha,epochs)\n",
    "print()\n",
    "\n",
    "#VALIDATION\n",
    "y_pred_val_blobs = predict(X_val_blobs,theta_blobs)\n",
    "\n",
    "#TESTING\n",
    "y_pred_test_blobs = predict(X_test_blobs,theta_blobs)\n",
    "\n",
    "simple_val_blobs_accr = metrics.accuracy_score(y_val_blobs, y_pred_val_blobs)*100\n",
    "print(\"Validation Accuracy = \",round(simple_val_blobs_accr,2),\"%\")\n",
    "\n",
    "simple_test_blobs_accr = metrics.accuracy_score(y_test_blobs, y_pred_test_blobs)*100\n",
    "print(\"Test Accuracy = \",round(simple_test_blobs_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "Blobs dataset is a linearly separable dataset. Based on above results, the Logistic Regression model is giving good accuracy (100%) on Blobs dataset for both validation and test data. The model is not overfitting as both validation and tests sets has 100% accuracy. \n",
    "\n",
    "The model gave best accuracy when following hyperparameters are used:<br>\n",
    "Learning rate (alpha) = 0.01 <br>\n",
    "Epochs = 10,000 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Shallow Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks are a computational model that is similar to the way human brain process information using biological neural networks [2]. Neural networks consists of input nodes which taken in the feature vectors. There can be one or more hidden layers which perform intermediate computation. These hidden layers can consist of one or more hidden nodes. The output layer uses activation function such as sigmoid to map the result to the required output. Similar to a simple machine learning mode, neural network also have weights and biases in each layer of the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, Logistic Regression with Stochastic Gradient Descent algorithm is implemented using a shallow Neural network with a single hidden layer. In forward propogation, we use the weights and bias of the hidden layer to get the weighted sum for the selected training case. In backpropogation, we propogate the errors from the output through the network and calculate the derivatives of weights and bias with respect to cost [3]. Using Stochastic Gradient Descent, we adjust the weight and bias values in the entire architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, early stopping is implemented for neural networks. In this technique, while training the model we check the performace of the validation set with current model parameters at regular intervals. If the current performance is better, we save the model performance and when we finish training, instead of returning the final parameters, we return the last saved parameters [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn(X,Y,a,epochs,neurons,X_val,y_val):\n",
    "    # The fit_nn function takes in the features list and labels from the train dataset \n",
    "    # along with the learning rate (alpha) and epochs and the number of neurons in the \n",
    "    # hidden layer. The features and labels of the validation set is also taken as  \n",
    "    # input. Stochastic Gradient descent(SGD) is implemented to update model parameters   \n",
    "    # - weights and bias. Using early stopping, the last saved weight and bias values   \n",
    "    # are returned from this function.\n",
    "    \n",
    "    #initialization\n",
    "    alpha = a\n",
    "    max_iterations = epochs\n",
    "    hidden_nodes = neurons\n",
    "    \n",
    "    np.random.seed(10) #for ensuring data can be reproduced\n",
    "    \n",
    "    a_hidden = np.empty((hidden_nodes,1))\n",
    "    w_sum = np.empty((hidden_nodes,1))\n",
    "    \n",
    "    #Intializing weights and bias with random values\n",
    "    w_input = np.random.rand(hidden_nodes,len(X.columns))\n",
    "    w_hidden = np.random.rand(hidden_nodes,1)\n",
    "    b_input = np.random.rand(1,hidden_nodes)\n",
    "    b_hidden = np.random.rand()\n",
    "        \n",
    "    del_w_input = np.empty((hidden_nodes,len(X.columns)))\n",
    "    del_w_hidden = np.empty((hidden_nodes,1))\n",
    "    del_b_input = np.empty((1,hidden_nodes))\n",
    "    del_z_input = np.empty((hidden_nodes,1))\n",
    " \n",
    "    current_accuracy= 0\n",
    "    previous_accuracy= 0\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        #selecting 1 random index from the training example for SGD\n",
    "        random_index = np.random.randint(0, len(X), 1) \n",
    "        \n",
    "        #extracting 1 row from training example based on index found above \n",
    "        x = X.iloc[random_index,:] \n",
    "        y = Y.iloc[random_index,:]\n",
    "        \n",
    "        x=x.squeeze() #removing dimension from feature list X\n",
    "        y=np.array(y)\n",
    "\n",
    "        #Forward propogation - Hidden Layer calculation\n",
    "        for i in range(hidden_nodes):\n",
    "            w_sum[i] = weighted_sum(x,w_input[i,:],b_input[:,i])\n",
    "            a_hidden[i] = sigmoid(w_sum[i])\n",
    "        \n",
    "        #Forward propogation - Output Layer calculation\n",
    "        z = sigmoid(weighted_sum(a_hidden,w_hidden,b_hidden))\n",
    "        if (z>=0.5):  \n",
    "            y_hat = 1\n",
    "        else: y_hat = 0\n",
    "        \n",
    "        #Backward Propogation - output layer calculation\n",
    "        del_z_hidden = y_hat - y\n",
    "        for i in range(hidden_nodes):\n",
    "            del_w_hidden[i] = del_z_hidden * a_hidden[i]\n",
    "        del_b_hidden = del_z_hidden\n",
    "        \n",
    "        #Backward Propogation - hidden layer calculation\n",
    "        for i in range(len(w_sum)):\n",
    "            z = w_sum[i]\n",
    "            sigmoid_prime = sigmoid(z)*(1-sigmoid(z))\n",
    "            del_z_input[i] = sigmoid_prime * del_z_hidden * w_hidden[i]\n",
    "\n",
    "        x = np.array(x)\n",
    "        x = x.reshape(1,len(X.columns))\n",
    "\n",
    "        for i in range(hidden_nodes):\n",
    "            for j in range(len(X.columns)):\n",
    "                del_w_input[i][j] = del_z_input[i] * x[0][j]\n",
    "    \n",
    "        del_b_input = w_sum\n",
    "        \n",
    "        #Gradient Descent calculation\n",
    "        w_input = w_input - alpha * del_w_input\n",
    "        w_hidden = w_hidden - alpha * del_w_hidden\n",
    "        \n",
    "        for i in range(hidden_nodes):\n",
    "            b_input[:,i] = b_input[:,i] - alpha * del_b_input[i,:]\n",
    "        b_hidden = b_hidden - alpha * del_b_hidden\n",
    "        \n",
    "        #Implementing Early Stopping to avoid Overfitting\n",
    "        if(_ % 1000 == 0):\n",
    "            theta = [w_input,b_input,w_hidden,b_hidden]\n",
    "            \n",
    "            #testing Validation data with currect weight & bias values\n",
    "            y_pred = predict_nn(X_val,tuple(theta)) \n",
    "            \n",
    "            current_accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "            if(current_accuracy > previous_accuracy):\n",
    "                saved_w_input = w_input\n",
    "                saved_b_input = b_input\n",
    "                saved_w_hidden = w_hidden\n",
    "                saved_b_hidden = b_hidden\n",
    "            \n",
    "    return saved_w_input, saved_b_input, saved_w_hidden, saved_b_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(X,theta):\n",
    "    # The predict_nn function takes in the feature list passed to the function. \n",
    "    # The theta value in the argument is a list which contains the model parameters \n",
    "    # - weights and bias for all layers. This function returns the predicted label \n",
    "    # value based on the probability returned by the sigmoid function.\n",
    "\n",
    "    #extracting weights and bias from theta\n",
    "    w_input = theta[0]\n",
    "    b_input = theta[1]\n",
    "    w_hidden = theta[2]\n",
    "    b_hidden = theta[3]\n",
    "    \n",
    "    a_hidden = np.empty((len(w_input),1))\n",
    "    y_hat = np.empty((len(X),1))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        x = X.iloc[i,:]\n",
    "        x=x.squeeze()\n",
    "        \n",
    "        #Forward propogation - Hidden Layer calculation\n",
    "        for j in range(len(a_hidden)):\n",
    "            a_hidden[j] = sigmoid(weighted_sum(x,w_input[j,:],b_input[:,j]))\n",
    "            \n",
    "        #Forward propogation - Output Layer calculation\n",
    "        z = sigmoid(weighted_sum(a_hidden,w_hidden,b_hidden))\n",
    "        if (z>=0.5):\n",
    "            y_hat[i] = 1\n",
    "        else: y_hat[i] = 0\n",
    "\n",
    "    return y_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "Validation Accuracy =  100.0 %\n",
      "Test Accuracy =  97.37 %\n"
     ]
    }
   ],
   "source": [
    "## For Blobs dataset\n",
    "alpha = 0.01\n",
    "epochs = 10000\n",
    "neurons_in_hidden_layer = 2\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_blobsNN = fit_nn(X_train_blobs,y_train_blobs,alpha,epochs,\n",
    "                       neurons_in_hidden_layer,X_val_blobs,y_val_blobs)\n",
    "\n",
    "#VALIDATION\n",
    "y_pred_val_blobsNN = predict_nn(X_val_blobs,theta_blobsNN)\n",
    "\n",
    "#TESTING\n",
    "y_pred_test_blobsNN = predict_nn(X_test_blobs,theta_blobsNN)\n",
    "\n",
    "nn_val_blobs_accr = metrics.accuracy_score(y_val_blobs, y_pred_val_blobsNN)*100\n",
    "print(\"Validation Accuracy = \",round(nn_val_blobs_accr,2),\"%\")\n",
    "\n",
    "nn_test_blobs_accr = metrics.accuracy_score(y_test_blobs, y_pred_test_blobsNN)*100\n",
    "print(\"Test Accuracy = \",round(nn_test_blobs_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For linearly seperable data such as blobs dataset, the Logistic Regression model implemented with shallow neural network is giving 100% accuracy for validation data and has 97% accuracy for test data. The performance of the shallow neural network is a bit lower (97%) than that of the simple logistic regression model which gave 100% accuracy for test data as well. \n",
    "\n",
    "The model gave best accuracy when following hyperparameters were used:<br>\n",
    "Learning rate (alpha) = 0.01 <br>\n",
    "Epochs = 10,000 <br>\n",
    "Number of neurons in the hidden layer = 2 <br>\n",
    "Validation accuracy for Early stopping is checked at every 1,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "Validation Accuracy =  90.2 %\n",
      "Test Accuracy =  86.67 %\n"
     ]
    }
   ],
   "source": [
    "## For Moons dataset\n",
    "alpha = 0.001\n",
    "epochs = 10000\n",
    "neurons_in_hidden_layer = 2\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_moonsNN = fit_nn(X_train_moons,y_train_moons,alpha,epochs,\n",
    "                       neurons_in_hidden_layer,X_val_moons,y_val_moons)\n",
    "\n",
    "#VALIDATION\n",
    "y_pred_val_moonsNN = predict_nn(X_val_moons,theta_moonsNN)\n",
    "\n",
    "#TESTING\n",
    "y_pred_test_moonsNN = predict_nn(X_test_moons,theta_moonsNN)\n",
    "\n",
    "nn_val_moons_accr = metrics.accuracy_score(y_val_moons, y_pred_val_moonsNN)*100\n",
    "print(\"Validation Accuracy = \",round(nn_val_moons_accr,2),\"%\")\n",
    "\n",
    "nn_test_moons_accr = metrics.accuracy_score(y_test_moons, y_pred_test_moonsNN)*100\n",
    "print(\"Test Accuracy = \",round(nn_test_moons_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For linearly inseperable data such as Moons dataset, the Logistic Regression model implemented with shallow neural network is giving 90% accuracy on validation data and only 86% accuracy on test data. However, when compared to the performance of simple logistic regression model implementation for the same data, we see a improvement in the accuracy of the test data from 83% to 86%. However, the accuracy of the validation set seems to have decreased from 92% in simple implementation to 90% in the neural network implementation. \n",
    "\n",
    "The model gave best accuracy when following hyperparameters were used:<br>\n",
    "Learning rate (alpha) = 0.001 <br>\n",
    "Epochs = 10,000 <br>\n",
    "Number of neurons in the hidden layer = 2 <br>\n",
    "Validation accuracy for Early stopping is checked at every 1,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enhancement implemented below for neural network implementation of Logistic Regression with Stochastic Gradient model is **Backprop with Momentum Algorithm**. Stochastic Gradient descent has issues traversing ravines which usually occur near the local minimas. Ravines are areas where the surface curves more steeply in one dimension as compared to others. Stochastic gradient tends to oscillate across the slope of ravines and only  slow progressing towards the local optima [5]. Using momentum we use previous changes in the parameter values to infulence the current direction of parameter updates [6] which helps stochastic gradient to accelerate in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_opt(X,Y,a,epochs,neurons,X_val,y_val):\n",
    "    # The fit_nn_opt function takes in the features list and labels from the train \n",
    "    # dataset along with the learning rate (alpha) and epochs and the number of \n",
    "    # neurons in the hidden layer. The features and labels of the validation set is \n",
    "    # also taken as input. Stochastic Gradient descent is implemented to update model \n",
    "    # parameters - weights and bias. Using early stopping, the last saved weight and \n",
    "    # bias values are returned from this function. For backprop with momentum, a new\n",
    "    # hyperparameter beta1 introduced which is set to 0.9 in below implementation.  \n",
    "    \n",
    "    #initialization\n",
    "    alpha = a\n",
    "    max_iterations = epochs\n",
    "    hidden_nodes = neurons\n",
    "    \n",
    "    np.random.seed(23)\n",
    "    \n",
    "    a_hidden = np.empty((hidden_nodes,1))\n",
    "    w_sum = np.empty((hidden_nodes,1))\n",
    "        \n",
    "    w_input = np.random.rand(hidden_nodes,len(X.columns))\n",
    "    w_hidden = np.random.rand(hidden_nodes,1)\n",
    "\n",
    "    b_input = np.random.rand(1,hidden_nodes)\n",
    "    b_hidden = np.random.rand()\n",
    "    \n",
    "    del_w_input = np.empty((hidden_nodes,len(X.columns)))\n",
    "    del_w_hidden = np.empty((hidden_nodes,1))\n",
    "    del_b_input = np.empty((1,hidden_nodes))\n",
    "    del_z_input = np.empty((hidden_nodes,1))\n",
    "    \n",
    "    V_del_w_input = np.zeros((hidden_nodes,len(X.columns)))\n",
    "    V_del_w_hidden = np.zeros((hidden_nodes,1))\n",
    "    V_del_b_input = np.zeros((1,hidden_nodes))\n",
    "    V_del_b_hidden = 0\n",
    "    \n",
    "    beta1 = 0.9\n",
    "\n",
    "    current_accuracy=0\n",
    "    previous_accuracy=0\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        t = _\n",
    "        #selecting 1 random training example for Stochastic Gradient Descent\n",
    "        random_index = np.random.randint(0, len(X), 1) \n",
    "        \n",
    "        x = X.iloc[random_index,:]\n",
    "        y = Y.iloc[random_index,:]\n",
    "        \n",
    "        x=x.squeeze()\n",
    "        y=np.array(y)\n",
    "\n",
    "        #Forward propogation - Hidden Layer calculation\n",
    "        for i in range(hidden_nodes):\n",
    "            w_sum[i] = weighted_sum(x,w_input[i,:],b_input[:,i])\n",
    "            a_hidden[i] = sigmoid(w_sum[i])\n",
    "        \n",
    "        #Forward propogation - Output Layer calculation\n",
    "        z = sigmoid(weighted_sum(a_hidden,w_hidden,b_hidden))\n",
    "        if (z>=0.5):\n",
    "            y_hat = 1\n",
    "        else: y_hat = 0\n",
    "        \n",
    "        #Backward Propogation - output layer calculation\n",
    "        del_z_hidden = y_hat - y\n",
    "        for i in range(hidden_nodes):\n",
    "            del_w_hidden[i] = del_z_hidden * a_hidden[i]\n",
    "        del_b_hidden = del_z_hidden\n",
    "        \n",
    "        #Momentum - output layer calculation\n",
    "        V_del_w_hidden = (1 - beta1) * del_w_hidden + beta1 * V_del_w_hidden\n",
    "        V_del_b_hidden = (1-beta1) * del_b_hidden + beta1 * V_del_b_hidden\n",
    "        \n",
    "        #Backward Propogation - hidden layer calculation\n",
    "        for i in range(len(w_sum)):\n",
    "            z = w_sum[i]\n",
    "            sigmoid_prime = sigmoid(z)*(1-sigmoid(z))\n",
    "            del_z_input[i] = sigmoid_prime * del_z_hidden * w_hidden[i]\n",
    "\n",
    "        x = np.array(x)\n",
    "        x = x.reshape(1,len(X.columns))\n",
    "\n",
    "        for i in range(hidden_nodes):\n",
    "            for j in range(len(X.columns)):\n",
    "                del_w_input[i][j] = del_z_input[i] * x[0][j]\n",
    "    \n",
    "        del_b_input = w_sum.T\n",
    "        \n",
    "        #Momentum - hidden layer calculation\n",
    "        V_del_w_input = (1-beta1)*del_w_input + beta1 * V_del_w_input\n",
    "        V_del_b_input = (1-beta1)*del_b_input + beta1 * V_del_b_input\n",
    "\n",
    "        #Gradient Descent calculation\n",
    "        w_input = w_input - alpha * V_del_w_input\n",
    "        w_hidden = w_hidden - alpha * V_del_w_hidden\n",
    "        \n",
    "        b_hidden = b_hidden - alpha * V_del_b_hidden\n",
    "        b_input = b_input - alpha * V_del_b_input\n",
    "        \n",
    "        #Implementing Early Stopping to avoid Overfitting\n",
    "        if(_ % 1000 == 0):\n",
    "            theta = [w_input,b_input,w_hidden,b_hidden]\n",
    "            \n",
    "            #testing Validation data with currect weight & bias values\n",
    "            y_pred = predict_nn(X_val,tuple(theta)) \n",
    "            \n",
    "            current_accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "            if(current_accuracy > previous_accuracy):\n",
    "                saved_w_input = w_input\n",
    "                saved_b_input = b_input\n",
    "                saved_w_hidden = w_hidden\n",
    "                saved_b_hidden = b_hidden\n",
    "            \n",
    "    return saved_w_input, saved_b_input, saved_w_hidden, saved_b_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "Validation Accuracy =  100.0 %\n",
      "Test Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "## For Blobs dataset\n",
    "alpha = 0.01\n",
    "epochs = 10000\n",
    "neurons_in_hidden_layer = 2\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_blobsNN = fit_nn_opt(X_train_blobs,y_train_blobs,alpha,epochs,\n",
    "                           neurons_in_hidden_layer,X_val_blobs,y_val_blobs)\n",
    "\n",
    "#print(\"***VALIDATION***\")\n",
    "y_pred_val_blobsNN = predict_nn(X_val_blobs,theta_blobsNN)\n",
    "\n",
    "#print(\"***TESTING***\")\n",
    "y_pred_test_blobsNN = predict_nn(X_test_blobs,theta_blobsNN)\n",
    "\n",
    "opt_val_blobs_accr = metrics.accuracy_score(y_val_blobs, y_pred_val_blobsNN)*100\n",
    "print(\"Validation Accuracy = \",round(opt_val_blobs_accr,2),\"%\")\n",
    "\n",
    "opt_test_blobs_accr = metrics.accuracy_score(y_test_blobs, y_pred_test_blobsNN)*100\n",
    "print(\"Test Accuracy = \",round(opt_test_blobs_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For linearly seperable data such as blobs dataset, the Logistic Regression model implemented with shallow neural network & backprop with momentum is giving good accuracy (100%) for both validation and test data. The model is not overfitting as both validation and tests sets has 100% accuracy. Thus, we can see the model is giving similar performance to that of simple logistic regression model for the same dataset.\n",
    "\n",
    "The model gave best accuracy when following hyperparameters are used:<br>\n",
    "Learning rate (alpha) = 0.01<br>\n",
    "Epochs = 10,000 <br>\n",
    "Number of neurons in the hidden layer = 2 <br>\n",
    "Beta for momentum calculation = 0.9 <br>\n",
    "Validation accuracy for Early stopping is checked at every 1,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "Validation Accuracy =  94.12 %\n",
      "Test Accuracy =  88.33 %\n"
     ]
    }
   ],
   "source": [
    "## For Moons dataset\n",
    "alpha = 0.001\n",
    "epochs = 10000\n",
    "neurons_in_hidden_layer = 2\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_moonsNN = fit_nn_opt(X_train_moons,y_train_moons,alpha,epochs,\n",
    "                           neurons_in_hidden_layer,X_val_moons,y_val_moons)\n",
    "\n",
    "y_pred_val_moonsNN = predict_nn(X_val_moons,theta_moonsNN)\n",
    "\n",
    "y_pred_test_moonsNN = predict_nn(X_test_moons,theta_moonsNN)\n",
    "\n",
    "opt_val_moons_accr = metrics.accuracy_score(y_val_moons, y_pred_val_moonsNN)*100\n",
    "print(\"Validation Accuracy = \",round(opt_val_moons_accr,2),\"%\")\n",
    "\n",
    "opt_test_moons_accr = metrics.accuracy_score(y_test_moons, y_pred_test_moonsNN)*100\n",
    "print(\"Test Accuracy = \",round(opt_test_moons_accr,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For linearly inseperable data such as Moons dataset, the Logistic Regression model implemented with shallow neural network & backprop with momentum is giving 94% accuracy on validation data and 88% accuracy on test data. The model is not overfitting as accuracy of both validation and tests sets is around 90%. However, when compared to the performance of simple logistic regression model & shallow neural netowork implementation for the same data, we see accuracy has improved with implementation of backprop with momentum to 88% on the test data from 83% of simple implementation & 86% of neural net implementation. The accuracy of the validation set seems to have improved as well from 92% & 90% in simple implementation and neural net implementation respectively to 94% when momentum is implemented.\n",
    "\n",
    "The model gave best accuracy when following hyperparameters are used:<br>\n",
    "Learning rate (alpha) = 0.001 <br>\n",
    "Epochs = 10,000 <br>\n",
    "Number of neurons in the hidden layer = 2 <br>\n",
    "Beta for momentum calculation = 0.9 <br>\n",
    "Validation accuracy for Early stopping is checked at every 1,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAINING MODEL***\n",
      "Validation Accuracy on train data =  55.62 %\n",
      "Test Accuracy on train data =  54.39 %\n",
      "Test Accuracy on test data =  52.4 %\n"
     ]
    }
   ],
   "source": [
    "#For images dataset\n",
    "alpha = 0.01\n",
    "epochs = 10000\n",
    "neurons_in_hidden_layer = 2\n",
    "\n",
    "print(\"***TRAINING MODEL***\")\n",
    "theta_img = fit_nn_opt(X_train_img,y_train_img,alpha,epochs,\n",
    "                       neurons_in_hidden_layer,X_val_img,y_val_img)\n",
    "\n",
    "#VALIDATION\n",
    "y_pred_val_img = predict_nn(X_val_img,theta_img)\n",
    "\n",
    "#TESTING on train data\n",
    "y_pred_test_img = predict_nn(X_test_img,theta_img)\n",
    "\n",
    "#TESTING on test data\n",
    "test_y_pred_img = predict_nn(test_X_scaled,theta_img)\n",
    "\n",
    "opt_val_img_accr = metrics.accuracy_score(y_val_img, y_pred_val_img)*100\n",
    "print(\"Validation Accuracy on train data = \",round(opt_val_img_accr,2),\"%\")\n",
    "\n",
    "opt_test_img_accr = metrics.accuracy_score(y_test_img, y_pred_test_img)*100\n",
    "print(\"Test Accuracy on train data = \",round(opt_test_img_accr,2),\"%\")\n",
    "\n",
    "opt_test_data_accr = metrics.accuracy_score(test_y, test_y_pred_img)*100\n",
    "print(\"Test Accuracy on test data = \",round(opt_test_data_accr,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "For CIFAR image dataset, the Logistic Regression model implemented with shallow neural network & backprop with momentum is giving around 55% accuracy on validation data and 54% on test data from training dataset and 52% on the test dataset. The model is underfitting as accuracy of both validation and tests sets is around 55%. The performance of the model has same as that of the shallow neural network so there is no improvment in accuracy after implementing backprop with momentum for image dataset.\n",
    "\n",
    "The model gave best accuracy when following hyperparameters are used:<br>\n",
    "Learning rate (alpha) = 0.01 <br>\n",
    "Epochs = 10,000 <br>\n",
    "Number of neurons in the hidden layer = 2 <br>\n",
    "Beta for momentum calculation = 0.9 <br>\n",
    "Validation accuracy for Early stopping is checked at every 1,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Niklas Donges. (2020, December 5). Gradient Descent: An Introduction To 1 Of Machine Learningâ€™s Most Popular Algorithms. Retrieved from https://builtin.com/data-science/gradient-descent <br>\n",
    "[2] David Fumo. (2017, August 4). A Gentle Introduction to Neural Networks Series - Part 1. Retrieved from https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc#:~:text=A%20feedforward%20neural%20network%20is,or%20loops%20in%20the%20network <br>\n",
    "[3] Dr. Michael Madden (2021). Deep Learning [Lecture notes Week 3]. Retrieved from https://nuigalway.blackboard.com/ <br>\n",
    "[4] Dr. Michael Madden (2021). Deep Learning [Lecture notes Week 4]. Retrieved from https://nuigalway.blackboard.com/ <br>\n",
    "[5] Sebastian Ruder. (2016, January 19). An overview of gradient descent optimization algorithms. Retrieved from https://ruder.io/optimizing-gradient-descent/index.html#momentum <br>\n",
    "[6] Dr. Michael Madden (2021). Deep Learning [Lecture notes Week 5]. Retrieved from https://nuigalway.blackboard.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
